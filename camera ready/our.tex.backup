There are some modifications which had to be done in order to adapt \textit{CluStream} in Spark. Working with Spark means worrking distributed computing and, thus, the algorithm has to be able to work in parallel. Both parts (online and offline) were adapted.

\subsection{CluStreamOnline class (online phase)}

Two processes were modified: processing the stream and updating the micro-clusters. As this adaptation uses Spark Streaming, the points coming from the stream are processed in batches at user specified time intervals. This contrasts with the original methodology which indicates to process point by point. The main difference with the batch processing method is that now points laying in current micro-clusters are processed before than the ones that are not, this also includes updating the micro-clusters before processint the points laying out. The reason for this is that as part of the strategy chosen to parallelize the algorithm, the micro-clusters are maintained locally and processing the outliers is also performed locally because deleting and merging requires to modify the micro-clusters for every point.

\subsubsection{Finding nearest micro-cluster}

The maintenance of the micro-clusters starts with this operation. After initialization (described in \cite{clustreamOrig}) is performed, finding the nearest micro-clusters for all the points is the very first thing to be done for every new batch of data.

\begin{algorithm}
 \caption{Find nearest micro-cluster.}\label{alg:assign}
 \begin{algorithmic}[1]
  \Require{\textit{rdd: RDD[breeze.linalg.Vector[Double]], mcInfo: Array[(McI,Int)]}| \textit{rdd} is an RDD containing data points and \textit{mcInfo} is the collection of the micro-clusters information.} 
  \Ensure{\textit{rdd: RDD[(Int, breeze.linalg.Vector[Double])]} | returns a tuple of the point itself and the unique ID of the nearest micro-cluster.}
  \vspace{10pt}
  \ForAll {$p \in rdd$}
  \State $minDistance \gets Double.PositiveInfinity$
  \State $minIndex \gets Int.MaxValue$
  \ForAll {$mc \in mcInfo$}
  \State $distance \gets squaredDistance(p, mc_1.centroid)$
  \If {$distance \leq minDistance$}
  \State $minDistance \gets distance$
  \State $minIndex \gets mc_2$
  \EndIf
  \EndFor
  \State $p = (minIndex,p)$
  \EndFor
  \Return $rdd$
 \end{algorithmic}
\end{algorithm}

Finding the nearest micro-clusters is an operation of complexity $O(n*q*d)$, where $n$ is the number of points, $q$ the number of micro-clusters and $d$ the dimension of the points; $q$ and $d$ remain constant during runtime but $n$ might vary. Algorithm \ref{alg:assign} describes a simple search for the minimum distance for every point in the RDD to the micro-clusters. This is also a good opportunity to show how this works using Spark and \textit{Scala}:

\
\begin{lstlisting}[language=Scala, tabsize=2, breaklines=true,basicstyle=\footnotesize,frame=lines,numbers=left]
def assignToMicroCluster(rdd: RDD[Vector[Double]], mcInfo: Array[(MicroClusterInfo, Int)]): RDD[(Int, Vector[Double])] = {
    rdd.map { a =>
      var minDist = Double.PositiveInfinity
      var minIndex = Int.MaxValue
      for (mc <- mcInfo) {
        val dist = squaredDistance(a, mc._1.centroid)
        if (dist < minDist) {
          minDist = dist
          minIndex = mc._2
        }}
      (minIndex, a)
    }}
\end{lstlisting}


Spark uses this $map$ operation to serialize the function passed so that all nodes in the cluster get the same instruction, this is exactly how computations are parallelized  within this framework. At this point, every node performs this operation to find the nearest micro-cluster for all the points they locally have.



\subsubsection{Processing points}\label{procpoints}

The points are separated in two: points within micro-clusters and outliers, it is possible to compute the necessary information from them to update the micro-clusters. It is important to perform this step before handling the outliers because this adaptation process the points for batches of data and not points individually as they arrive, and the reasons are:

\begin{itemize}
 \item Every point in a batch is treated equally in terms of temporal properties as this batch gets distributed among the cluster with the same time stamp and there is no constant communication among nodes.
\item The process of handling outliers involves deleting and merging micro-clusters for every outlier, modifying the micro-clusters' structure. A sequential implementation was preferred to reduce communication costs.
\end{itemize}

These two points are some of the key differences between the original $CluStream$ method and this adaptation. For the original, it is possible to handle point by point as each have different clear time stamps.


\subsubsection{Handling outliers}\label{handlingoutliers}


First the micro-clusters which are safe to delete are determined, then the outliers can be handled. In general, there are three possible scenarios for outliers:

\begin{itemize}
 \item If the point lies within the restriction regarding the RMSD for its nearest micro-cluster in the array  $newMicroClusters$, the point is added to it. This stores all newly created micro-clusters.
 \item If the point does not lie within any of the new micro-clusters, then it replaces a micro-cluster from the $safeDelete$ array, while there are safe-to-delete micro-clusters.
 \item If none of the previous scenarios are viable, then the two micro-clusters that are closest to each other get merged, freeing one spot to create the new micro-cluster. This is the the most computationally expensive scenario. The function $getTwoClosestMicroClusters()$ has a complexity of $O(p_{m}d\cdot \frac{q!}{2!(q-2)!})$, where $p_m$ is the number of outliers that require a merge, $d$ the dimension of the points, and $q$ the number of micro-clusters.
\end{itemize}


\begin{algorithm}[h]
 \caption{handle outliers.}\label{alg:handleoutliers}
 \begin{algorithmic}[1]

  \vspace{10pt}
  \State $ j \gets 0$
  \ForAll {$p \in dataOut$}
  \State $distance,mcID \gets $
  \item[] $getMinDistanceFromIDs(newMicroClusters,p_2)$
  \If {$distance < t * mcInfo[mcID]_1.rmsd$}
  \State $addPointToMicroCluster(mcID,p_2)$
  \Else \If {$safeDelete[j].isDefined$}
  \State $replaceMicroCluster(safeDelete[j],p_2)$
  \State $newMicroClusters.append(j)$
  \State $j \gets j + 1$
  \Else
  \State $index1,index2 \gets $
  \item[] $getTwoClosestMicroClusters(keepOrMerge)$
  \State $mergeMicroClusters(index1,index2)$
  \State $replaceMicroClusters(index2,p_2)$
  \State $newMicroClusters.append(j)$
  \State $j \gets j + 1$
  \EndIf
  \EndIf
  \EndFor
 \end{algorithmic}
\end{algorithm}

It is important in the procedure described in Algorithm \ref{alg:handleoutliers} to locally update the $mcInfo$ every time a point is added to a micro-cluster, two micro clusters are merged and when a new micro-cluster is created. There could be a lot of change, depending on the outliers, and this loop requires up-to-date information for each iteration, otherwise merges and the RMSD check would be inaccurate.


\subsection{CluStream class (offline phase)}

Using a \textit{weighted K-Means} approach, as described in \cite{clustreamOrig} was not directly possible, and for that reason, a new adaptation had to be done in order to achive similar results.


\textit{The fakeKMeans solution:}

The original \textit{CluStream} method suggests to use a slightly modified version of K-Means, a version for which one can initialize the seeds (initial clusters) by sampling from the micro-clusters' centroids taking into account the number of points each micro-cluster has and for which one can use these centroids as weighted input points. These weights, again, are related to the number of points they absorbed. Spark's (current) implementation of K-Means does allow to initialize the seeds but unfortunately it is not possible to predefine the weights for the input points.

In order to solve this issue, a new version of K-Means needs to be implemented. This version uses, in fact, Spark's own version, but to overcome the problem of not being able to define the weights at the beginning, this new version uses as input many points sampled from the micro-clusters' centroids.