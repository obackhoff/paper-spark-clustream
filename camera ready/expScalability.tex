We test the scalability with respect to data dimensionality and number of microclusters, using data generated by a \textit{Random Radial Basis Function} generator.

The scalability tests are performed in two different scenarios: one being an analysis of how it scales for different number of attributes (dimensions of the data points) using only 20 micro-clusters and the other one using 200 micro-clusters as the number of attributes and the number of final clusters for a specific purpose are two key factors which determine the complexity of \textit{Spark-CluStream}. The speed of the stream is controlled for 10000 points for every batch of data because it is easier to test the scalability when many computations have to be done. An application using Spark streaming assigns one core exclusively to handle the stream, therefore the number of processors mentioned in here is the total, but the real number of processors used for the computations is that number minus one.


\begin{figure*}[!ht]
        \begin{minipage}[l]{1.0\columnwidth}
            \centering
             \includegraphics[width=0.9\columnwidth]{./styles/perf20-2.png}
            \caption{Dimension: $d$ = 2}
            \label{fig:perf20-2}
        \end{minipage}
        \hfill{}
        \begin{minipage}[r]{1.0\columnwidth}
            \centering
            \includegraphics[width=0.9\columnwidth]{./styles/perf20-100.png}
            \caption{Dimension: $d$ = 100}\label{fig:perf20-100}
        \end{minipage}
        \captionsetup{labelformat=empty}
	\caption{Scalability-dimensionality comparison for Stream speed = 10000 and q = 20}
	 \label{fig:scal20}
\end{figure*}
\addtocounter{figure}{-1}

Figure \ref{fig:perf20-2} shows that using only 20 micro-clusters and 2 dimensions has poor scalability, not even being able to perform twice as fast as for a single processor (2 in total). Even for this high speed streaming, one processor is enough to process the batches of data before a new batch is processed, meaning that the setup is stable.

\begin{figure*}[!ht]
        \begin{minipage}[l]{1.0\columnwidth}
            \centering
             \includegraphics[width=0.9\columnwidth]{./styles/perf200-2.png}
            \caption{Dimension: $d$ = 2}
            \label{fig:perf200-2}
        \end{minipage}
        \hfill{}
        \begin{minipage}[r]{1.0\columnwidth}
            \centering
            \includegraphics[width=0.9\columnwidth]{./styles/perf200-100.png}
            \caption{Dimension: $d$ = 100}\label{fig:perf200-100}
        \end{minipage}
        \captionsetup{labelformat=empty}     
        \caption{Scalability-dimensionality comparison for Stream speed = 10000 and q = 200}
        \label{fig:scal200}
\end{figure*}

\addtocounter{figure}{-1}
Increasing the dimensionality of the points increases the computational effort needed to process the points in every batch of data and here is where \textit{Spark-CluStream} shows its scalability, which is almost linear\footnote{By linear scalability does not mean it scales with a 1 to 1 ratio, but rather linearly proportional.} for up to 16-17 processors, as it can be seen in Figure \ref{fig:perf20-100}.  From the average processing time per batch, it can be seen that from 32 to 40 processors it does not improve much anymore and the speedup does not increase quasi-linearly anymore. Here a total of 9 processors were required to stabilize \textit{Spark-CluStream}. 


Interestingly, increasing the number of micro-clusters by a factor of 10 for 2 attributes resulted in good scalability, similarly to the scenario with 20 micro-clusters and 100 attributes. Here a total of 8 processors were enough for a stable run, as shown in Figure \ref{fig:perf200-2}.



Finally, when the number of clusters and the number of attributes are both increased significantly, Figure \ref{fig:perf200-100} shows for \textit{Spark-CluStream} quasi-linear scalability but this time only up to about 8-9 processors. After that point, the speedup slows down showing almost no improvement after 16 processors. This test never reached a stable configuration.


\subsection{Performance}

In this section, the scalability of \textit{Spark-CluStream} is compared to that of \textit{StreamDM-CluStream} and Spark's \textit{Streaming K-Means} using the Spark cluster setup for $q=20$ and $d=100$, for the $CluStream$ method. Also, a test on a single machine is performed.


\begin{figure}[h!]
 \centering
 \includegraphics[scale=0.45]{./styles/perfComp100.png}
 \caption{Processing time comparison: $q=20$, $d=100$}
 \label{fig:perfComp100}
\end{figure}

When it comes to higher dimensions, \textit{Spark-CluStream} shows a significant improvement over \textit{StreamDM-CluStream}, which never got to the point were it was stable (below the 1 second mark), as shown in Figure \ref{fig:perfComp100}, it seems to scale as fast as \textit{Spark-CluStream} but it was not enough even with 40 processors. 


Another interesting comparison, is the processing time per batch of data for a single machine, using a real dataset such as the \textit{Network Intrusion}. Here, communication is less of an issue as all the partitions lie in the same share memory space, and still there are 4 virtual cores in disposition for the algorithms to run. 

The test was performed using a stream speed of 2000 points per batch and with a horizon $H=1$, to match one of the validation tests.

\begin{figure}[h!]
 \centering
 \includegraphics[scale=0.47]{./styles/singlemachine.png}
 \caption{Processing time comparison for a single machine: $q=50$, $d=34$}
 \label{fig:singlemachine}
\end{figure}

The results shown in Figure \ref{fig:singlemachine} are quite remarkable. As \textit{StreamDM-CluStream} shows a very significant disadvantage when using greater numbers of micro-clusters and higher dimensions.

For this single machine test, \textit{Spark-CluStream} was about 18 times faster on average than \textit{StreamDM-CluStream} and about two times slower than \textit{Streaming K-Means} on average.

Another consideration to be made, is that  \textit{Spark-CluStream} saves a snapshot for every batch of data, having to write to disk, while the other two algorithms never access the disk for this matter.


